{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML-Youtube analytics.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx4i-AH5y7Le"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTgxx-kCzq-W"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark \n",
        "import warnings\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sc = pyspark.SparkContext()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfrIuoMFDKRy"
      },
      "source": [
        "# **Training**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98KFcEYqzvyi"
      },
      "source": [
        "Data_set = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/content/drive/Shareddrives/YT-data/clean_tweet.csv')\n",
        "Data_set = Data_set.dropna()\n",
        "(train_set, val_set, test_set) = Data_set.randomSplit([0.98, 0.01, 0.01], seed = 2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NapOiiQy3bTC"
      },
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, CountVectorizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "from pyspark.ml.feature import  VectorAssembler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN6CJ3A435Ol"
      },
      "source": [
        "\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "\n",
        "C_V = CountVectorizer(vocabSize=2**16, inputCol=\"tokens\", outputCol='cv')\n",
        "\n",
        "idf = IDF(inputCol='cv', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "indexer = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
        "\n",
        "lr = LogisticRegression()\n",
        "\n",
        "IDF_pipeline = Pipeline(stages=[tokenizer, A_V, idf, indexer, lr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJY2MISB4JYA"
      },
      "source": [
        "\n",
        "IDF_pipelineFit = IDF_pipeline.fit(train_set)\n",
        "IDF_predictions = IDF_pipelineFit.transform(val_set)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rtu0GRs61Cx"
      },
      "source": [
        "train_accuracy = IDF_predictions.filter(IDF_predictions.label == IDF_predictions.prediction).count() / float(IDF_predictions.count())\n",
        "\n",
        "print(train_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWdYKp_xEYn5"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlhHeDmR_cmJ"
      },
      "source": [
        "IDF_test_predictions = IDF_pipelineFit.transform(test_set)\n",
        "test_accuracy = IDF_test_predictions.filter(IDF_test_predictions.label == IDF_test_predictions.prediction).count() / float(test_set.count())\n",
        "\n",
        "# print accuracy, roc_auc\n",
        "print(test_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QA0AyHcA1k1"
      },
      "source": [
        "# Level of engagement \n",
        "### positivity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PkUGKxpBCrU"
      },
      "source": [
        "API_comment = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/content/drive/Shareddrives/YT-data/API_data.csv')\n",
        "\n",
        "data = API_comment.selectExpr( \"video_id as video_id\",\"comment_text as text\")\n",
        "from pyspark.sql.functions import col, concat_ws\n",
        "data = data.withColumn(\"text\", concat_ws(\",\",col(\"text\")))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWLk4u4BePfR"
      },
      "source": [
        "### DATA CLEANING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6I6ht4Xmd0Hs"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "toknizer = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "\n",
        "def data_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped_text = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped_text)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lowercase = letters.lower()\n",
        "    words = toknizer.tokenize(lowercase)\n",
        "    return (\" \".join(words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LGf6r4Cd1NQ"
      },
      "source": [
        " \n",
        "data = data.rdd.map(lambda x : (x[0] , data_cleaner(x[1])) ).toDF().selectExpr( \"_1 as video_id\",\"_2 as text\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Bokan8AJ1C"
      },
      "source": [
        "test_predictions = IDF_pipelineFit.transform(data.limit(250000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxJHl3FAwL9z"
      },
      "source": [
        "out = test_predictions[[\"video_id\" , \"prediction\"]]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-gp1B8RH5dA"
      },
      "source": [
        "count = out.rdd.map(lambda x : (x[0] , 1)).reduceByKey(lambda x ,y : x+y)\n",
        "psitive = out.rdd.map(lambda x : (x[0] , x[1])).reduceByKey(lambda x ,y : x+y)\n",
        "positive_count= psitive.join(count)\n",
        "positivity = positive_count.map(lambda x : (x[0] , x[1][0] / x[1][1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhiWQ68Bje6B"
      },
      "source": [
        "Avg. likes for positivity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hcYnl1tjfdv"
      },
      "source": [
        "API_like_dislike_data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('/content/drive/Shareddrives/YT-data/API_stat.csv')\n",
        "\n",
        "ld = API_like_dislike_data.selectExpr( \"video_id as video_id\",\"likeCount as likes\" ,\"dislikeCount as dislikes\" , \"viewCount as view\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIOHcZlaH5T6"
      },
      "source": [
        "likes_num = ld.rdd.map(lambda x : (x[0] , x[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6jKKKq14_u5"
      },
      "source": [
        "psitivity_reacts = positivity.join(likes_num)\n",
        "count_pos = psitivity_reacts.map(lambda x: (round(x[1][0],1) ,1)).reduceByKey(lambda x  ,y : x+y)\n",
        "commulative_likes = psitivity_reacts.map(lambda x: (x[1][0] ,x[1][1])).reduceByKey(lambda x  ,y : x+y)\n",
        "likes_count= count_pos.join(commulative_likes)\n",
        "pos_avgreacts = likes_count.map(lambda x : (x[0] , x[1][1] / x[1][0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vuv7PnGKzkt-"
      },
      "source": [
        "sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "621d72txtqLH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}